{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_saliency, visualize_cam\n",
    "from keras import Model\n",
    "from model import unet, aunet\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'example/data'\n",
    "model_path = 'example/model'\n",
    "result_path = 'example/result'\n",
    "mask_path = 'example/mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(data_root, './')\n",
    "method_path = os.path.join(model_path, 'unet_.hdf5')\n",
    "method_net = unet(pretrained_weights=method_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'a6/up_sampling2d_1/ResizeBilinear:0' shape=(?, 20, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_net.get_layer('a6').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x14b5c61e828>,\n",
       " <keras.layers.core.Lambda at 0x14b5c61e9e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c633ba8>,\n",
       " <keras.layers.core.Dropout at 0x14b5e8b28d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c6339b0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x14b5e8aff28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c609ef0>,\n",
       " <keras.layers.core.Dropout at 0x14b5c6e4dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c6f9908>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x14b5c6f9a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c714780>,\n",
       " <keras.layers.core.Dropout at 0x14b5c76f7f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c76f4a8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x14b5c7c5cf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c78d5f8>,\n",
       " <keras.layers.core.Dropout at 0x14b5c7ff208>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c7ffd30>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x14b5c7fff98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c81bd30>,\n",
       " <keras.layers.core.Dropout at 0x14b5c8768d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c876c50>,\n",
       " <keras.layers.convolutional.Conv2DTranspose at 0x14b5c892908>,\n",
       " <keras.layers.merge.Concatenate at 0x14b5c8e9f98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5c8e9e48>,\n",
       " <keras.layers.core.Dropout at 0x14b5f925128>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5f925358>,\n",
       " <keras.layers.convolutional.Conv2DTranspose at 0x14b5f925cc0>,\n",
       " <keras.layers.merge.Concatenate at 0x14b5f96bf60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5f9cbe48>,\n",
       " <keras.layers.core.Dropout at 0x14b5f9f48d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5f9f4470>,\n",
       " <keras.layers.convolutional.Conv2DTranspose at 0x14b5f9f4ba8>,\n",
       " <keras.layers.merge.Concatenate at 0x14b5fa43fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5fa817f0>,\n",
       " <keras.layers.core.Dropout at 0x14b5fac2b70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5fac2080>,\n",
       " <keras.layers.convolutional.Conv2DTranspose at 0x14b5fb186a0>,\n",
       " <keras.layers.merge.Concatenate at 0x14b5fae4400>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5fb3d9b0>,\n",
       " <keras.layers.core.Dropout at 0x14b5fb8a470>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5fb8a978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x14b5fbc8ba8>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(method_net.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_74/Elu:0' shape=(?, 160, 224, 16) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_net.get_layer(index=40).output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_net_a6 = Model(inputs=method_net.input, outputs=method_net.get_layer('a6').output)\n",
    "layer_net_a7 = Model(inputs=method_net.input, outputs=method_net.get_layer('a7').output)\n",
    "layer_net_a8 = Model(inputs=method_net.input, outputs=method_net.get_layer('a8').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "def min_max_scaler(a):\n",
    "    return (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "\n",
    "# crop roi for unet prediction\n",
    "def crop_roi(img):\n",
    "    test_img = np.rot90(min_max_scaler(img))\n",
    "    crop_img = test_img[130:130 + 160, 50:50 + 224]\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NII_fileList = []\n",
    "for dirName, subdirList, fileList in os.walk(input_path):\n",
    "    for filename in fileList:\n",
    "        if \".nii\" in filename.lower():\n",
    "            # print filename\n",
    "            NII_fileList.append(os.path.join(dirName, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detect in F:\\GitHub\\DICOM\\example\\data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 20.42it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "input_3:0 is both fed and fetched.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-03932b2357d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mcrop_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrop_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0msaliency_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize_saliency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m41\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrop_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\vis\\visualization\\saliency.py\u001b[0m in \u001b[0;36mvisualize_saliency\u001b[1;34m(model, layer_idx, filter_indices, seed_input, backprop_modifier, grad_modifier)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mActivationMaximization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     ]\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvisualize_saliency_with_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_modifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\vis\\visualization\\saliency.py\u001b[0m in \u001b[0;36mvisualize_saliency_with_losses\u001b[1;34m(input_tensor, losses, seed_input, grad_modifier)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_grads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_modifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_modifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mchannel_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\vis\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, seed_input, max_iter, input_modifiers, grad_modifier, callbacks, verbose)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# 0 learning phase for 'test'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mcomputed_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputed_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mnamed_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3287\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3288\u001b[0m         session != self._session):\n\u001b[1;32m-> 3289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   3220\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3221\u001b[0m     \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3222\u001b[1;33m     \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3223\u001b[0m     \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3224\u001b[0m     \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \"\"\"\n\u001b[0;32m   1488\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1444\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1446\u001b[1;33m             session._session, options_ptr)\n\u001b[0m\u001b[0;32m   1447\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: input_3:0 is both fed and fetched."
     ]
    }
   ],
   "source": [
    "for file in tqdm(NII_fileList, desc='Detect in {}'.format(os.path.abspath(input_path))):\n",
    "    filename = os.path.basename(file)\n",
    "    #_, _ = detect_file(file, model, layer_net)\n",
    "    data = nib.load(file)\n",
    "    \n",
    "    width, height, frame_num = data.shape\n",
    "    matrix = data.get_data()\n",
    "    #unet_matrix = bet_unet(matrix, layer_net, threshold=0.2)\n",
    "    img_batch = np.transpose(matrix, (2, 0, 1))\n",
    "    #mask_batch = unet_batch_predict(img_batch, layer_net, threshold=0.2)\n",
    "    batch, h, w = img_batch.shape\n",
    "    crop_batch = []\n",
    "    for img in img_batch:\n",
    "        crop_img = crop_roi(img)\n",
    "        crop_batch.append(crop_img)\n",
    "    crop_batch = np.array(crop_batch)\n",
    "    crop_batch = np.reshape(crop_batch, crop_batch.shape + (1,))\n",
    "    \n",
    "saliency_map = visualize_saliency(model=method_net, layer_idx=41, seed_input=crop_batch[0], filter_indices=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 160, 224, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = utils.find_layer_idx(method_net, 'a6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_75/Sigmoid:0' shape=(?, 160, 224, 1) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_net.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
